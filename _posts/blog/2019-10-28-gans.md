---
layout: blog
small-title: GANs
visible: 1
big-title: Generative Adversarial Networks
feature-img: "/images/blog/Algorithm.jpg"
thumbnail: "/images/blog/thumbs/Algorithm.jpg"
tags: [GAN, machine learning, python]
teaser: Common problems while training GANs
intro: GANs are known for being very fiddly to train and optimise. Here are a few things I've learnt while working with them.
---



<<<<<<< HEAD:_posts/blog/2019-10-28-gans.md
GAN HACKS: https://github.com/soumith/ganhacks

Complexify the NN a little so it can learn the EE
Too few neurons in a layer can restrict the representation that the network learns, causing under-fitting. Too many neurons can cause over-fitting because the network will "memorize" the training data.

I was working with too few neurons for a while and the GAN was not learning anything!





Numerical data (lat/long) is a lot harder to learn than categorical data.


Batch size smaller/bigger

Clip_size

Learning rathe

n_critic
=======

>>>>>>> e2984491311246f25be4e8763a45c39d1c5f4dd1:_posts/blog/2019-10-28-test.md

# Typical problems with GANs:
- Mode collapse
- Convergence failure


# Identify common problems
How to identify these problems while training is in progress to save time?

## Normal learning:
### Signs:
Gentle oscillations of G and D. Sometimes goes up sometimes goes down. It shows it is learning something.
D loss for real and fake samples is about the same at or around 0.5, G loss is slightly higher between 0.5 and 2.0

Interesting article about NN not learning:
https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn
	if you're seeing a loss that's bigger than 1, it's likely your model is very skewed.
	Ideally around 0.5 (50% ) at start


After a while, D converges, stops improving. At this point, G does not learn anything new.

## Mode collapse:
### Signs:
### Causes:
Mode collapse in GANs is a bit of a mystery. It's often not an imbalance between D or G but rather that G fails to explore different values. G converges at an equilibrium with D.

Always had mode collapse with WGAN... I don't know why

## Convergence failure:
<<<<<<< HEAD:_posts/blog/2019-10-28-gans.md
### Signs:
• G loss slowly increases
• D loss slowly decreases
=======
### Signs: 
- G loss slowly increases
- D loss slowly decreases
>>>>>>> e2984491311246f25be4e8763a45c39d1c5f4dd1:_posts/blog/2019-10-28-test.md

### Reason:
D is better than G and spots the 'garbage' every time. G is not good enough - it tries different values and is being busted systematically.
### Causes:
Imbalance between G and D. D is too strong, G is too weak.



If G is too strong, it gets away with crap output. Need to always make D better, even if it means G will take longer to learn, it will learn!

```python

```
