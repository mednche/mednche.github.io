---
layout: blog
small-title: GANs
visible: 1
big-title: Generative Adversarial Networks
feature-img: "/images/blog/Algorithm.jpg"
thumbnail: "/images/blog/thumbs/Algorithm.jpg"
tags: [GAN, machine learning, python]
teaser: Common problems while training GANs
visible: 1
intro: GANs are known for being very fiddly to train and optimise. Here are a few things I've learnt while working with them.
---
Coming soon
<!--

These [GAN HACKS](https://github.com/soumith/ganhacks) have been very useful along the way.

- Complexify the NN a little so it can learn the EE.
- Too few neurons in a layer can restrict the representation that the network learns, causing under-fitting. 
- Too many neurons can cause over-fitting because the network will "memorize" the training data.

I was working with too few neurons for a while and the GAN was not learning anything!


Numerical data (lat/long) is a lot harder to learn than categorical data.


- Batch size smaller/bigger

- Clip_size

- Learning rate

- n_critic


# Typical problems with GANs:
- Mode collapse
- Convergence failure


# Identify common problems
How to identify these problems while training is in progress to save time?

## Normal learning:
### Signs:
Gentle oscillations of G and D. Sometimes goes up sometimes goes down. It shows it is learning something.
D loss for real and fake samples is about the same at or around 0.5, G loss is slightly higher between 0.5 and 2.0

Interesting article about NN not learning:
https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn
	if you're seeing a loss that's bigger than 1, it's likely your model is very skewed.
	Ideally around 0.5 (50% ) at start


After a while, D converges, stops improving. At this point, G does not learn anything new.

## Mode collapse:
### Signs:
### Causes:
Mode collapse in GANs is a bit of a mystery. It's often not an imbalance between D or G but rather that G fails to explore different values. G converges at an equilibrium with D.

I always had mode collapse with WGAN... I don't know why.

## Convergence failure:

### Signs:
• G loss slowly increases
• D loss slowly decreases


### Signs: 
- G loss slowly increases
- D loss slowly decreases


### Reason:
D is better than G and spots the 'garbage' every time. G is not good enough - it tries different values and is being busted systematically.
### Causes:
Imbalance between G and D. D is too strong, G is too weak.


If G is too strong, it gets away with 'garbage' output. Need to always make D better, even if it means G will take longer to learn. It will learn!

```python

```
-->
