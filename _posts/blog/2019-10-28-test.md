
---
layout: blog
small-title: ECCA symposium
big-title: Environmental Criminology and Crime Analysis international symposium
feature-img: "/images/blog/ECCA.jpg"
thumbnail: "/images/blog/thumbs/ECCA.jpg"
tags: [conference, criminology, travel]
teaser: June 2019 - Guangzhou (China)
intro: In June, I travelled to Guangzhou (China) to present my PhD research at the ECCA 2019 symposium.
---



layout: blog
small-title: GANs
big-title: Generative Adversarial Networks
feature-img: "/images/blog/Algorithm.jpg"
thumbnail: "/images/blog/thumbs/Algorithm.jpg"
tags: [GAN, machine learning, python]
teaser: Common problems while training
intro: GANs are known for being very fiddly to train and optimise. Here are a few things I've learnt while working with them.



# Typical problems with GANs:
- Mode collapse
- Convergence failure


# Identify common problems
How to identify these problems while training is in progress to save time?

## Normal learning:
### Signs:
Gentle oscillations of G and D. Sometimes goes up sometimes goes down. It shows it is learning something.
D loss for real and fake samples is about the same at or around 0.5, G loss is slightly higher between 0.5 and 2.0

## Mode collapse:
### Signs:
### Causes:
Mode collapse in GANs is a bit of a mystery. It's often not an imbalance between D or G but rather that G fails to explore different values. G converges at an equilibrium with D.

## Convergence failure:
### Signs: 
• G loss slowly increases
• D loss slowly decreases

### Reason: 
D is better than G and spots the 'garbage' every time. G is not good enough - it tries different values and is being busted systematically.
### Causes:
Imbalance between G and D. D is too strong, G is too weak.


```python

```
