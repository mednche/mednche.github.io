
---
layout: blog
small-title: GANs
big-title: Generative Adversarial Networks
feature-img: "/images/blog/Algorithm.jpg"
thumbnail: "/images/blog/thumbs/Algorithm.jpg"
tags: [GAN, machine learning, python]
teaser: Common problems while training
intro: GANs are known for being very fiddly to train and optimise. Here are a few things I've learnt while working with them.
---


# Typical problems with GANs:
- Mode collapse
- Convergence failure


# Identify common problems
How to identify these problems while training is in progress to save time?

## Normal learning:
### Signs:
- G loss goes down slowly
- D loss go up slowly
A few variation on the way. 
D loss for real and fake samples is about the same at or around 0.5, G loss is slightly higher between 0.5 and 2.0

## Mode collapse:
## Signs:
## Causes:
Mode collapse in GANs is a bit of a mystery. It's often not an imbalance between D or G but rather that G fails to explore different values. G converges at an equilibrium with D.

## Convergence failure:
### Signs: 
• G loss slowly increases
• D loss slowly decreases

## Reason: 
D is better than G and spots the 'garbage' every time. G is not good enough - it tries different values and is being busted systematically.
## Causes:
Imbalance between G and D. D is too strong, G is too weak.


```python

```
